{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORvwib-YSR-p"
      },
      "source": [
        "**BASIC MNIST EXAMPLE WITH MLP**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.8.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQE5-QpCSLjM",
        "outputId": "64fd7431-03a4-4636-9afb-a6cf522c118b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu==2.8.3\n",
            "  Downloading tensorflow_gpu-2.8.3-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 497.9 MB 33 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (0.2.0)\n",
            "Collecting tensorflow-estimator<2.9,>=2.8\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 66.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (1.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (0.27.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (14.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (1.50.0)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (3.19.6)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 61.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.8.3) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu==2.8.3) (0.38.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow-gpu==2.8.3) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (2.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.3) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.8.0 which is incompatible.\n",
            "tensorflow 2.9.2 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.8.0 which is incompatible.\n",
            "tensorflow 2.9.2 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.8.0 tensorboard-2.8.0 tensorflow-estimator-2.8.0 tensorflow-gpu-2.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMCWDmEZaREl",
        "outputId": "d5e9a100-4b71-4d5c-8388-e7825f38c590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 30 17:46:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGX8RGwHSSTr"
      },
      "source": [
        "Imports..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D8qeBuPiSSdc"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import RandomTranslation as RT\n",
        "from keras.layers import RandomRotation as RR\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9uBGifSSqd"
      },
      "source": [
        "Define batch size, number of epochs and number of classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iaWFWm8mSTBj"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 200\n",
        "num_classes=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKwjPVkoSTLr"
      },
      "source": [
        "Load MNIST and transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ9D1F8FSTWW",
        "outputId": "efa300cf-4f6d-4cd5-d1d0-529ad9b674b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "training set (60000, 28, 28)\n",
            "test set (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('training set', x_train.shape)\n",
        "print('test set', x_test.shape)\n",
        "\n",
        "x_train = x_train.reshape(60000, 28,28,1)\n",
        "x_test = x_test.reshape(10000, 28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24bIWSBXSThW"
      },
      "source": [
        "Define the NN topology, a sequential model with 2 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pFy6tdc0STtb"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(RR(factor=0.05,fill_mode='constant'))\n",
        "model.add(RT(height_factor=(-0.1,0.1),width_factor=(-0.1,0.1),fill_mode='constant'))\n",
        "model.add(Reshape(target_shape=(784,), input_shape=(28,28,1)))\n",
        "model.add(GN(0.3))\n",
        "model.add(Dense(1024, input_shape=(784,)))\n",
        "model.add(BN())\n",
        "model.add(GN(0.3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1024))\n",
        "model.add(BN())\n",
        "model.add(GN(0.3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1024))\n",
        "model.add(BN())\n",
        "model.add(GN(0.3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "#model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDjl8exTTDZ"
      },
      "source": [
        "Define an optimizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LTRuAT1FTTOw"
      },
      "outputs": [],
      "source": [
        "sgd=SGD(learning_rate=0.01, decay=1e-6, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHhBHWFjTTYy"
      },
      "source": [
        "Compile the model, define loss and link the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8W8KCPtcTTii"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBOKdV6MTTtA"
      },
      "source": [
        "Finally, train the model and evaluate over the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLPsSdVDTT37",
        "outputId": "3f333a84-aae2-4ec9-aeb5-3529ba87771f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "469/469 [==============================] - 11s 14ms/step - loss: 0.8131 - accuracy: 0.7289 - val_loss: 0.1644 - val_accuracy: 0.9467\n",
            "Epoch 2/200\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4499 - accuracy: 0.8541 - val_loss: 0.1143 - val_accuracy: 0.9634\n",
            "Epoch 3/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3725 - accuracy: 0.8795 - val_loss: 0.0938 - val_accuracy: 0.9706\n",
            "Epoch 4/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3311 - accuracy: 0.8941 - val_loss: 0.0876 - val_accuracy: 0.9698\n",
            "Epoch 5/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2983 - accuracy: 0.9045 - val_loss: 0.0743 - val_accuracy: 0.9745\n",
            "Epoch 6/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2840 - accuracy: 0.9095 - val_loss: 0.0701 - val_accuracy: 0.9762\n",
            "Epoch 7/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2679 - accuracy: 0.9129 - val_loss: 0.0609 - val_accuracy: 0.9797\n",
            "Epoch 8/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2510 - accuracy: 0.9189 - val_loss: 0.0571 - val_accuracy: 0.9815\n",
            "Epoch 9/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2426 - accuracy: 0.9218 - val_loss: 0.0557 - val_accuracy: 0.9818\n",
            "Epoch 10/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2327 - accuracy: 0.9249 - val_loss: 0.0526 - val_accuracy: 0.9822\n",
            "Epoch 11/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2263 - accuracy: 0.9280 - val_loss: 0.0498 - val_accuracy: 0.9821\n",
            "Epoch 12/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2175 - accuracy: 0.9294 - val_loss: 0.0487 - val_accuracy: 0.9831\n",
            "Epoch 13/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2120 - accuracy: 0.9316 - val_loss: 0.0472 - val_accuracy: 0.9842\n",
            "Epoch 14/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2056 - accuracy: 0.9339 - val_loss: 0.0474 - val_accuracy: 0.9848\n",
            "Epoch 15/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2016 - accuracy: 0.9347 - val_loss: 0.0429 - val_accuracy: 0.9857\n",
            "Epoch 16/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1958 - accuracy: 0.9367 - val_loss: 0.0444 - val_accuracy: 0.9847\n",
            "Epoch 17/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1943 - accuracy: 0.9375 - val_loss: 0.0431 - val_accuracy: 0.9848\n",
            "Epoch 18/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1875 - accuracy: 0.9393 - val_loss: 0.0400 - val_accuracy: 0.9872\n",
            "Epoch 19/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1815 - accuracy: 0.9415 - val_loss: 0.0389 - val_accuracy: 0.9866\n",
            "Epoch 20/200\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1830 - accuracy: 0.9420 - val_loss: 0.0388 - val_accuracy: 0.9862\n",
            "Epoch 21/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1766 - accuracy: 0.9434 - val_loss: 0.0368 - val_accuracy: 0.9874\n",
            "Epoch 22/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1730 - accuracy: 0.9436 - val_loss: 0.0397 - val_accuracy: 0.9868\n",
            "Epoch 23/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1704 - accuracy: 0.9453 - val_loss: 0.0379 - val_accuracy: 0.9861\n",
            "Epoch 24/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1677 - accuracy: 0.9456 - val_loss: 0.0370 - val_accuracy: 0.9856\n",
            "Epoch 25/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1655 - accuracy: 0.9456 - val_loss: 0.0348 - val_accuracy: 0.9874\n",
            "Epoch 26/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1660 - accuracy: 0.9470 - val_loss: 0.0352 - val_accuracy: 0.9884\n",
            "Epoch 27/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1629 - accuracy: 0.9475 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 28/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1594 - accuracy: 0.9484 - val_loss: 0.0382 - val_accuracy: 0.9854\n",
            "Epoch 29/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1609 - accuracy: 0.9483 - val_loss: 0.0350 - val_accuracy: 0.9870\n",
            "Epoch 30/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1609 - accuracy: 0.9484 - val_loss: 0.0348 - val_accuracy: 0.9875\n",
            "Epoch 31/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1537 - accuracy: 0.9499 - val_loss: 0.0336 - val_accuracy: 0.9880\n",
            "Epoch 32/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1548 - accuracy: 0.9494 - val_loss: 0.0305 - val_accuracy: 0.9885\n",
            "Epoch 33/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1543 - accuracy: 0.9499 - val_loss: 0.0317 - val_accuracy: 0.9882\n",
            "Epoch 34/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1493 - accuracy: 0.9513 - val_loss: 0.0356 - val_accuracy: 0.9881\n",
            "Epoch 35/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1494 - accuracy: 0.9523 - val_loss: 0.0321 - val_accuracy: 0.9878\n",
            "Epoch 36/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1461 - accuracy: 0.9528 - val_loss: 0.0334 - val_accuracy: 0.9881\n",
            "Epoch 37/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1432 - accuracy: 0.9536 - val_loss: 0.0326 - val_accuracy: 0.9878\n",
            "Epoch 38/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1429 - accuracy: 0.9534 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 39/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1407 - accuracy: 0.9545 - val_loss: 0.0357 - val_accuracy: 0.9871\n",
            "Epoch 40/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1398 - accuracy: 0.9546 - val_loss: 0.0310 - val_accuracy: 0.9887\n",
            "Epoch 41/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1383 - accuracy: 0.9554 - val_loss: 0.0319 - val_accuracy: 0.9874\n",
            "Epoch 42/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1377 - accuracy: 0.9558 - val_loss: 0.0303 - val_accuracy: 0.9895\n",
            "Epoch 43/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1383 - accuracy: 0.9549 - val_loss: 0.0291 - val_accuracy: 0.9893\n",
            "Epoch 44/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1382 - accuracy: 0.9549 - val_loss: 0.0273 - val_accuracy: 0.9898\n",
            "Epoch 45/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1321 - accuracy: 0.9568 - val_loss: 0.0322 - val_accuracy: 0.9885\n",
            "Epoch 46/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1345 - accuracy: 0.9565 - val_loss: 0.0314 - val_accuracy: 0.9881\n",
            "Epoch 47/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1346 - accuracy: 0.9560 - val_loss: 0.0320 - val_accuracy: 0.9879\n",
            "Epoch 48/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1331 - accuracy: 0.9569 - val_loss: 0.0280 - val_accuracy: 0.9899\n",
            "Epoch 49/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1308 - accuracy: 0.9573 - val_loss: 0.0285 - val_accuracy: 0.9905\n",
            "Epoch 50/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1315 - accuracy: 0.9587 - val_loss: 0.0283 - val_accuracy: 0.9895\n",
            "Epoch 51/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1269 - accuracy: 0.9588 - val_loss: 0.0281 - val_accuracy: 0.9894\n",
            "Epoch 52/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1269 - accuracy: 0.9592 - val_loss: 0.0276 - val_accuracy: 0.9893\n",
            "Epoch 53/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1256 - accuracy: 0.9589 - val_loss: 0.0258 - val_accuracy: 0.9907\n",
            "Epoch 54/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1276 - accuracy: 0.9582 - val_loss: 0.0285 - val_accuracy: 0.9896\n",
            "Epoch 55/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1245 - accuracy: 0.9596 - val_loss: 0.0294 - val_accuracy: 0.9889\n",
            "Epoch 56/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1234 - accuracy: 0.9597 - val_loss: 0.0295 - val_accuracy: 0.9894\n",
            "Epoch 57/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1238 - accuracy: 0.9600 - val_loss: 0.0256 - val_accuracy: 0.9908\n",
            "Epoch 58/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1203 - accuracy: 0.9605 - val_loss: 0.0263 - val_accuracy: 0.9903\n",
            "Epoch 59/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1215 - accuracy: 0.9608 - val_loss: 0.0275 - val_accuracy: 0.9893\n",
            "Epoch 60/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1189 - accuracy: 0.9604 - val_loss: 0.0295 - val_accuracy: 0.9897\n",
            "Epoch 61/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1192 - accuracy: 0.9603 - val_loss: 0.0279 - val_accuracy: 0.9884\n",
            "Epoch 62/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1228 - accuracy: 0.9606 - val_loss: 0.0283 - val_accuracy: 0.9901\n",
            "Epoch 63/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1180 - accuracy: 0.9625 - val_loss: 0.0268 - val_accuracy: 0.9904\n",
            "Epoch 64/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1191 - accuracy: 0.9616 - val_loss: 0.0271 - val_accuracy: 0.9900\n",
            "Epoch 65/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1197 - accuracy: 0.9612 - val_loss: 0.0268 - val_accuracy: 0.9900\n",
            "Epoch 66/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1157 - accuracy: 0.9635 - val_loss: 0.0264 - val_accuracy: 0.9911\n",
            "Epoch 67/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1179 - accuracy: 0.9621 - val_loss: 0.0254 - val_accuracy: 0.9911\n",
            "Epoch 68/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1125 - accuracy: 0.9634 - val_loss: 0.0276 - val_accuracy: 0.9889\n",
            "Epoch 69/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1209 - accuracy: 0.9607 - val_loss: 0.0252 - val_accuracy: 0.9905\n",
            "Epoch 70/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1153 - accuracy: 0.9628 - val_loss: 0.0252 - val_accuracy: 0.9911\n",
            "Epoch 71/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1159 - accuracy: 0.9627 - val_loss: 0.0254 - val_accuracy: 0.9909\n",
            "Epoch 72/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1123 - accuracy: 0.9635 - val_loss: 0.0271 - val_accuracy: 0.9899\n",
            "Epoch 73/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1151 - accuracy: 0.9624 - val_loss: 0.0248 - val_accuracy: 0.9914\n",
            "Epoch 74/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1142 - accuracy: 0.9635 - val_loss: 0.0266 - val_accuracy: 0.9901\n",
            "Epoch 75/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1089 - accuracy: 0.9650 - val_loss: 0.0246 - val_accuracy: 0.9912\n",
            "Epoch 76/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1092 - accuracy: 0.9648 - val_loss: 0.0243 - val_accuracy: 0.9920\n",
            "Epoch 77/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1061 - accuracy: 0.9655 - val_loss: 0.0238 - val_accuracy: 0.9909\n",
            "Epoch 78/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1098 - accuracy: 0.9642 - val_loss: 0.0243 - val_accuracy: 0.9904\n",
            "Epoch 79/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1066 - accuracy: 0.9650 - val_loss: 0.0248 - val_accuracy: 0.9907\n",
            "Epoch 80/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1093 - accuracy: 0.9653 - val_loss: 0.0239 - val_accuracy: 0.9912\n",
            "Epoch 81/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1078 - accuracy: 0.9653 - val_loss: 0.0230 - val_accuracy: 0.9914\n",
            "Epoch 82/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1083 - accuracy: 0.9655 - val_loss: 0.0259 - val_accuracy: 0.9903\n",
            "Epoch 83/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1077 - accuracy: 0.9657 - val_loss: 0.0230 - val_accuracy: 0.9915\n",
            "Epoch 84/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1085 - accuracy: 0.9652 - val_loss: 0.0252 - val_accuracy: 0.9905\n",
            "Epoch 85/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1057 - accuracy: 0.9650 - val_loss: 0.0234 - val_accuracy: 0.9914\n",
            "Epoch 86/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1041 - accuracy: 0.9663 - val_loss: 0.0223 - val_accuracy: 0.9919\n",
            "Epoch 87/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1048 - accuracy: 0.9667 - val_loss: 0.0245 - val_accuracy: 0.9907\n",
            "Epoch 88/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1029 - accuracy: 0.9661 - val_loss: 0.0237 - val_accuracy: 0.9912\n",
            "Epoch 89/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1044 - accuracy: 0.9659 - val_loss: 0.0215 - val_accuracy: 0.9919\n",
            "Epoch 90/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1021 - accuracy: 0.9665 - val_loss: 0.0240 - val_accuracy: 0.9908\n",
            "Epoch 91/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1012 - accuracy: 0.9672 - val_loss: 0.0257 - val_accuracy: 0.9905\n",
            "Epoch 92/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1036 - accuracy: 0.9668 - val_loss: 0.0245 - val_accuracy: 0.9903\n",
            "Epoch 93/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0996 - accuracy: 0.9672 - val_loss: 0.0223 - val_accuracy: 0.9910\n",
            "Epoch 94/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1027 - accuracy: 0.9674 - val_loss: 0.0250 - val_accuracy: 0.9904\n",
            "Epoch 95/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1015 - accuracy: 0.9669 - val_loss: 0.0244 - val_accuracy: 0.9917\n",
            "Epoch 96/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1041 - accuracy: 0.9660 - val_loss: 0.0223 - val_accuracy: 0.9916\n",
            "Epoch 97/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1039 - accuracy: 0.9671 - val_loss: 0.0243 - val_accuracy: 0.9914\n",
            "Epoch 98/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1024 - accuracy: 0.9670 - val_loss: 0.0231 - val_accuracy: 0.9917\n",
            "Epoch 99/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0994 - accuracy: 0.9678 - val_loss: 0.0245 - val_accuracy: 0.9903\n",
            "Epoch 100/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1010 - accuracy: 0.9673 - val_loss: 0.0229 - val_accuracy: 0.9913\n",
            "Epoch 101/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1006 - accuracy: 0.9672 - val_loss: 0.0226 - val_accuracy: 0.9913\n",
            "Epoch 102/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0979 - accuracy: 0.9683 - val_loss: 0.0229 - val_accuracy: 0.9913\n",
            "Epoch 103/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1029 - accuracy: 0.9665 - val_loss: 0.0224 - val_accuracy: 0.9920\n",
            "Epoch 104/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0992 - accuracy: 0.9678 - val_loss: 0.0218 - val_accuracy: 0.9922\n",
            "Epoch 105/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0958 - accuracy: 0.9685 - val_loss: 0.0231 - val_accuracy: 0.9918\n",
            "Epoch 106/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0982 - accuracy: 0.9683 - val_loss: 0.0239 - val_accuracy: 0.9912\n",
            "Epoch 107/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0980 - accuracy: 0.9678 - val_loss: 0.0232 - val_accuracy: 0.9921\n",
            "Epoch 108/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0974 - accuracy: 0.9676 - val_loss: 0.0219 - val_accuracy: 0.9925\n",
            "Epoch 109/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0984 - accuracy: 0.9682 - val_loss: 0.0225 - val_accuracy: 0.9918\n",
            "Epoch 110/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0968 - accuracy: 0.9688 - val_loss: 0.0208 - val_accuracy: 0.9927\n",
            "Epoch 111/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0968 - accuracy: 0.9685 - val_loss: 0.0228 - val_accuracy: 0.9916\n",
            "Epoch 112/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0960 - accuracy: 0.9683 - val_loss: 0.0208 - val_accuracy: 0.9922\n",
            "Epoch 113/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0959 - accuracy: 0.9692 - val_loss: 0.0205 - val_accuracy: 0.9927\n",
            "Epoch 114/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0944 - accuracy: 0.9689 - val_loss: 0.0212 - val_accuracy: 0.9920\n",
            "Epoch 115/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0962 - accuracy: 0.9684 - val_loss: 0.0203 - val_accuracy: 0.9927\n",
            "Epoch 116/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0898 - accuracy: 0.9710 - val_loss: 0.0202 - val_accuracy: 0.9926\n",
            "Epoch 117/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0946 - accuracy: 0.9695 - val_loss: 0.0220 - val_accuracy: 0.9926\n",
            "Epoch 118/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0929 - accuracy: 0.9700 - val_loss: 0.0209 - val_accuracy: 0.9913\n",
            "Epoch 119/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0939 - accuracy: 0.9705 - val_loss: 0.0239 - val_accuracy: 0.9906\n",
            "Epoch 120/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0948 - accuracy: 0.9695 - val_loss: 0.0247 - val_accuracy: 0.9909\n",
            "Epoch 121/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0919 - accuracy: 0.9708 - val_loss: 0.0242 - val_accuracy: 0.9918\n",
            "Epoch 122/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0935 - accuracy: 0.9697 - val_loss: 0.0220 - val_accuracy: 0.9920\n",
            "Epoch 123/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0908 - accuracy: 0.9703 - val_loss: 0.0217 - val_accuracy: 0.9924\n",
            "Epoch 124/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0939 - accuracy: 0.9704 - val_loss: 0.0232 - val_accuracy: 0.9916\n",
            "Epoch 125/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0913 - accuracy: 0.9705 - val_loss: 0.0221 - val_accuracy: 0.9922\n",
            "Epoch 126/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0921 - accuracy: 0.9704 - val_loss: 0.0220 - val_accuracy: 0.9926\n",
            "Epoch 127/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0893 - accuracy: 0.9711 - val_loss: 0.0232 - val_accuracy: 0.9917\n",
            "Epoch 128/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0929 - accuracy: 0.9701 - val_loss: 0.0216 - val_accuracy: 0.9919\n",
            "Epoch 129/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0876 - accuracy: 0.9712 - val_loss: 0.0208 - val_accuracy: 0.9928\n",
            "Epoch 130/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0904 - accuracy: 0.9708 - val_loss: 0.0227 - val_accuracy: 0.9923\n",
            "Epoch 131/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0913 - accuracy: 0.9708 - val_loss: 0.0201 - val_accuracy: 0.9930\n",
            "Epoch 132/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0881 - accuracy: 0.9706 - val_loss: 0.0228 - val_accuracy: 0.9921\n",
            "Epoch 133/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0891 - accuracy: 0.9714 - val_loss: 0.0201 - val_accuracy: 0.9929\n",
            "Epoch 134/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0891 - accuracy: 0.9707 - val_loss: 0.0194 - val_accuracy: 0.9930\n",
            "Epoch 135/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0860 - accuracy: 0.9714 - val_loss: 0.0188 - val_accuracy: 0.9934\n",
            "Epoch 136/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0915 - accuracy: 0.9704 - val_loss: 0.0204 - val_accuracy: 0.9932\n",
            "Epoch 137/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0875 - accuracy: 0.9717 - val_loss: 0.0205 - val_accuracy: 0.9930\n",
            "Epoch 138/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0887 - accuracy: 0.9714 - val_loss: 0.0205 - val_accuracy: 0.9931\n",
            "Epoch 139/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0869 - accuracy: 0.9715 - val_loss: 0.0200 - val_accuracy: 0.9935\n",
            "Epoch 140/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0876 - accuracy: 0.9718 - val_loss: 0.0202 - val_accuracy: 0.9923\n",
            "Epoch 141/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0896 - accuracy: 0.9708 - val_loss: 0.0213 - val_accuracy: 0.9922\n",
            "Epoch 142/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0881 - accuracy: 0.9714 - val_loss: 0.0212 - val_accuracy: 0.9926\n",
            "Epoch 143/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0850 - accuracy: 0.9728 - val_loss: 0.0189 - val_accuracy: 0.9937\n",
            "Epoch 144/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0853 - accuracy: 0.9724 - val_loss: 0.0199 - val_accuracy: 0.9934\n",
            "Epoch 145/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0878 - accuracy: 0.9711 - val_loss: 0.0185 - val_accuracy: 0.9932\n",
            "Epoch 146/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0865 - accuracy: 0.9710 - val_loss: 0.0189 - val_accuracy: 0.9926\n",
            "Epoch 147/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0856 - accuracy: 0.9719 - val_loss: 0.0210 - val_accuracy: 0.9929\n",
            "Epoch 148/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0833 - accuracy: 0.9727 - val_loss: 0.0199 - val_accuracy: 0.9935\n",
            "Epoch 149/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0859 - accuracy: 0.9719 - val_loss: 0.0199 - val_accuracy: 0.9932\n",
            "Epoch 150/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0832 - accuracy: 0.9734 - val_loss: 0.0198 - val_accuracy: 0.9929\n",
            "Epoch 151/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0868 - accuracy: 0.9718 - val_loss: 0.0181 - val_accuracy: 0.9937\n",
            "Epoch 152/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0884 - accuracy: 0.9711 - val_loss: 0.0200 - val_accuracy: 0.9935\n",
            "Epoch 153/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0873 - accuracy: 0.9720 - val_loss: 0.0217 - val_accuracy: 0.9922\n",
            "Epoch 154/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0828 - accuracy: 0.9735 - val_loss: 0.0209 - val_accuracy: 0.9927\n",
            "Epoch 155/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0860 - accuracy: 0.9718 - val_loss: 0.0188 - val_accuracy: 0.9935\n",
            "Epoch 156/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0791 - accuracy: 0.9735 - val_loss: 0.0198 - val_accuracy: 0.9930\n",
            "Epoch 157/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0828 - accuracy: 0.9736 - val_loss: 0.0190 - val_accuracy: 0.9927\n",
            "Epoch 158/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0829 - accuracy: 0.9728 - val_loss: 0.0183 - val_accuracy: 0.9935\n",
            "Epoch 159/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0839 - accuracy: 0.9723 - val_loss: 0.0203 - val_accuracy: 0.9925\n",
            "Epoch 160/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0816 - accuracy: 0.9731 - val_loss: 0.0186 - val_accuracy: 0.9939\n",
            "Epoch 161/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0837 - accuracy: 0.9732 - val_loss: 0.0214 - val_accuracy: 0.9927\n",
            "Epoch 162/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0823 - accuracy: 0.9733 - val_loss: 0.0203 - val_accuracy: 0.9930\n",
            "Epoch 163/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0809 - accuracy: 0.9743 - val_loss: 0.0198 - val_accuracy: 0.9931\n",
            "Epoch 164/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0844 - accuracy: 0.9729 - val_loss: 0.0206 - val_accuracy: 0.9930\n",
            "Epoch 165/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0816 - accuracy: 0.9736 - val_loss: 0.0189 - val_accuracy: 0.9934\n",
            "Epoch 166/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0815 - accuracy: 0.9740 - val_loss: 0.0188 - val_accuracy: 0.9933\n",
            "Epoch 167/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0825 - accuracy: 0.9729 - val_loss: 0.0183 - val_accuracy: 0.9940\n",
            "Epoch 168/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0831 - accuracy: 0.9725 - val_loss: 0.0186 - val_accuracy: 0.9932\n",
            "Epoch 169/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0830 - accuracy: 0.9732 - val_loss: 0.0206 - val_accuracy: 0.9930\n",
            "Epoch 170/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0805 - accuracy: 0.9740 - val_loss: 0.0189 - val_accuracy: 0.9939\n",
            "Epoch 171/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0817 - accuracy: 0.9730 - val_loss: 0.0187 - val_accuracy: 0.9940\n",
            "Epoch 172/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0806 - accuracy: 0.9733 - val_loss: 0.0191 - val_accuracy: 0.9937\n",
            "Epoch 173/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0803 - accuracy: 0.9740 - val_loss: 0.0207 - val_accuracy: 0.9926\n",
            "Epoch 174/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0801 - accuracy: 0.9734 - val_loss: 0.0195 - val_accuracy: 0.9937\n",
            "Epoch 175/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0807 - accuracy: 0.9733 - val_loss: 0.0181 - val_accuracy: 0.9940\n",
            "Epoch 176/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0809 - accuracy: 0.9733 - val_loss: 0.0205 - val_accuracy: 0.9931\n",
            "Epoch 177/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0785 - accuracy: 0.9743 - val_loss: 0.0195 - val_accuracy: 0.9932\n",
            "Epoch 178/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0818 - accuracy: 0.9732 - val_loss: 0.0188 - val_accuracy: 0.9940\n",
            "Epoch 179/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0802 - accuracy: 0.9735 - val_loss: 0.0206 - val_accuracy: 0.9928\n",
            "Epoch 180/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0795 - accuracy: 0.9743 - val_loss: 0.0202 - val_accuracy: 0.9932\n",
            "Epoch 181/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0793 - accuracy: 0.9743 - val_loss: 0.0194 - val_accuracy: 0.9935\n",
            "Epoch 182/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0779 - accuracy: 0.9742 - val_loss: 0.0195 - val_accuracy: 0.9927\n",
            "Epoch 183/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0770 - accuracy: 0.9745 - val_loss: 0.0194 - val_accuracy: 0.9933\n",
            "Epoch 184/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0834 - accuracy: 0.9734 - val_loss: 0.0201 - val_accuracy: 0.9934\n",
            "Epoch 185/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0787 - accuracy: 0.9742 - val_loss: 0.0200 - val_accuracy: 0.9931\n",
            "Epoch 186/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0792 - accuracy: 0.9742 - val_loss: 0.0201 - val_accuracy: 0.9930\n",
            "Epoch 187/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0781 - accuracy: 0.9744 - val_loss: 0.0200 - val_accuracy: 0.9930\n",
            "Epoch 188/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0765 - accuracy: 0.9743 - val_loss: 0.0203 - val_accuracy: 0.9929\n",
            "Epoch 189/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0797 - accuracy: 0.9746 - val_loss: 0.0190 - val_accuracy: 0.9935\n",
            "Epoch 190/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0788 - accuracy: 0.9743 - val_loss: 0.0208 - val_accuracy: 0.9925\n",
            "Epoch 191/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0774 - accuracy: 0.9752 - val_loss: 0.0202 - val_accuracy: 0.9930\n",
            "Epoch 192/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0761 - accuracy: 0.9754 - val_loss: 0.0201 - val_accuracy: 0.9931\n",
            "Epoch 193/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0785 - accuracy: 0.9744 - val_loss: 0.0184 - val_accuracy: 0.9936\n",
            "Epoch 194/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0765 - accuracy: 0.9750 - val_loss: 0.0196 - val_accuracy: 0.9933\n",
            "Epoch 195/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0763 - accuracy: 0.9751 - val_loss: 0.0185 - val_accuracy: 0.9933\n",
            "Epoch 196/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0760 - accuracy: 0.9753 - val_loss: 0.0191 - val_accuracy: 0.9936\n",
            "Epoch 197/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0737 - accuracy: 0.9763 - val_loss: 0.0183 - val_accuracy: 0.9943\n",
            "Epoch 198/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0748 - accuracy: 0.9755 - val_loss: 0.0191 - val_accuracy: 0.9933\n",
            "Epoch 199/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0753 - accuracy: 0.9749 - val_loss: 0.0198 - val_accuracy: 0.9934\n",
            "Epoch 200/200\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0761 - accuracy: 0.9748 - val_loss: 0.0189 - val_accuracy: 0.9940\n",
            "Test loss: 0.018850630149245262\n",
            "Test accuracy: 0.9940000176429749\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate over test\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_oBDiZbbO0G",
        "outputId": "1a6ef9cc-462e-4aed-ce7e-40dae1994a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K4uLWmxCbaq9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "1wlYhPH9bcz9",
        "outputId": "2b30ec14-ee5b-4785-8392-28f7a9e39a6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}